import torch
import os
from PIL import Image
from transformers import AutoTokenizer, CLIPImageProcessor
from peft import PeftModel
import argparse
import sys
sys.path.append("/root/KoLLaVA")
from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM

def main():
    parser = argparse.ArgumentParser(description="KoLLaVA ëª¨ë¸ ì¸í¼ëŸ°ìŠ¤ ì‹¤í–‰")
    parser.add_argument('--image_path', type=str, required=True, help='ì¶”ë¡ í•  ì´ë¯¸ì§€ ê²½ë¡œ')
    parser.add_argument('--question', type=str, required=True, help='ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸')
    parser.add_argument('--model_name', type=str, default='tabtoyou/KoLLaVA-KoVicuna-7b', help='KoLLaVA ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„')
    parser.add_argument('--checkpoint_path', type=str, default='/root/checkpoints/final', help='LoRA ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    args = parser.parse_args()

    # Tokenizer, ImageProcessor ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)
    image_processor = CLIPImageProcessor.from_pretrained("/root/KoLLaVA-KoVicuna-7b")

    # ëª¨ë¸ ë¡œë“œ
    print("KoLLaVA ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = LlavaLlamaForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    print("LoRA ì²´í¬í¬ì¸íŠ¸ ì ìš© ì¤‘...")
    model = PeftModel.from_pretrained(model, args.checkpoint_path)
    model.eval()

    # ì´ë¯¸ì§€ ë° í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    image = Image.open(args.image_path).convert("RGB")
    pixel_values = image_processor(images=image, return_tensors="pt")["pixel_values"].to("cuda")

    prompt = f"<image>\n{args.question}"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    # ëª¨ë¸ ì…ë ¥ í¬ë§· êµ¬ì„±
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    print("KoLLaVA ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            images=pixel_values,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.8
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("\nğŸ§  KoLLaVA ì‘ë‹µ:")
    print(response)

if __name__ == "__main__":
    main()
